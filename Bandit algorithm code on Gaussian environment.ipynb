{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Created by Arinze Lawrence Folarin\\nDate: 25th August, 2020\\nProject: An Analysis of Bandit Algorithms on Malaria Control.\\nTitle: AIMS master essay codes\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#AIMS_essay_codes\n",
    "'''Created by Arinze Lawrence Folarin\n",
    "Date: 25th August, 2020\n",
    "Project: An Analysis of Bandit Algorithms on Malaria Control.\n",
    "Title: AIMS master essay codes\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation and perfomance of Bandit Algorithms on Gaussian Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Modified code of: \n",
    "#2016-2018 Shangtong Zhang(zhangshangtong.cpp@gmail.com)             \n",
    "# 2016 Tian Jun(tianjun.cpp@gmail.com)                                \n",
    "# 2016 Artem Oboturov(oboturov@gmail.com)           \n",
    "# 2016 Kenta Shimada(hyperkentakun@gmail.com)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "%matplotlib inline\n",
    "matplotlib.use('TkAgg')\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "%matplotlib inline\n",
    "#matplotlib.use('Agg')\n",
    "\n",
    "\n",
    "\n",
    "class Bandit:\n",
    "    # @k_arm: # of arms\n",
    "    # @epsilon: probability for exploration in epsilon-greedy algorithm\n",
    "    # @initial: initial estimation for each action\n",
    "    # @step_size: constant step size for updating estimations\n",
    "    # @sample_averages: if True, use sample averages to update estimations instead of constant step size\n",
    "    # @UCB_param: if not None, use UCB algorithm to select action\n",
    "    # @gradient: if True, use gradient based bandit algorithm\n",
    "    # @gradient_baseline: if True, use average reward as baseline for gradient based bandit algorithm\n",
    "    # @decay1: if True,compute the value of epsilon using function 3\n",
    "    # @decay: if True,compute the value of epsilon using function 2\n",
    "    # @decay2: if True,compute the value of epsilon using function 1\n",
    "    def __init__(self, k_arm=10, epsilon=0., initial=0., step_size=0.1, sample_averages=False, UCB_param=None,\n",
    "                 gradient=False, decay=False,gradient_baseline=False, true_reward=0.,decay2=False,decay1=False,decay_eps=1.0,total_time=0):\n",
    "        self.k = k_arm\n",
    "        self.step_size = step_size\n",
    "        self.sample_averages = sample_averages\n",
    "        self.indices = np.arange(self.k)\n",
    "        self.time = 0\n",
    "        self.UCB_param = UCB_param\n",
    "        self.decay = decay\n",
    "        self.gradient = gradient\n",
    "        self.gradient_baseline = gradient_baseline\n",
    "        self.average_reward = 0\n",
    "        self.true_reward = true_reward\n",
    "        self.epsilon = epsilon\n",
    "        self.decay2 = decay2\n",
    "        self.decay_eps = decay_eps\n",
    "        self.decay1 = decay1\n",
    "        self.initial = initial\n",
    "        self.total_time =total_time\n",
    "\n",
    "    def reset(self):\n",
    "        # real reward for each action\n",
    "        self.q_true = np.random.randn(self.k) + self.true_reward\n",
    "\n",
    "        # estimation for each action\n",
    "        self.q_estimation = np.zeros(self.k) + self.initial  \n",
    "        # # of chosen times for each action\n",
    "        self.action_count = np.zeros(self.k)\n",
    "\n",
    "        self.best_action = np.argmax(self.q_true) \n",
    "\n",
    "        self.time = 0\n",
    "\n",
    "    # get an action for this bandit\n",
    "    def act(self):\n",
    "        #################################################\n",
    "        # DECAYING E-GREEDY FUNCTION 2\n",
    "        if self.decay:\n",
    "            self.decay_eps = 1/np.log(self.time+0.00001)\n",
    "            if np.random.rand()< self.decay_eps:\n",
    "                return np.random.choice(self.indices)\n",
    "        # DECAYING E-GREEDY FUNCTION 1\n",
    "        if self.decay2:\n",
    "            self.decay_eps = self.decay_eps*0.9\n",
    "            if np.random.rand()<self.decay_eps:\n",
    "                return np.random.choice(self.indices)\n",
    "        ################################################\n",
    "        # DECAYING E-GREEDY FUNCTION 3\n",
    "        if self.decay1:\n",
    "            num = (self.total_time-self.time)/self.total_time\n",
    "            r = np.max(num,0)\n",
    "            P_end = 0.1\n",
    "            self.decay_eps = (self.decay_eps - P_end)* r + P_end\n",
    "            if np.random.rand()<self.decay_eps:\n",
    "                return np.random.choice(self.indices)\n",
    "        ##########################################################\n",
    "        # E-GREEDY\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            #print(self.epsilon)\n",
    "            return np.random.choice(self.indices)\n",
    "        \n",
    "        #################################################################\n",
    "        # UCB METHOD\n",
    "        if self.UCB_param is not None:\n",
    "            UCB_estimation = self.q_estimation + \\\n",
    "                self.UCB_param * np.sqrt(np.log(self.time + 1) / (self.action_count + 1e-5))\n",
    "            q_best = np.max(UCB_estimation) ## The UCB formular\n",
    "            return np.random.choice(np.where(UCB_estimation == q_best)[0]) ## Returns the action\n",
    "        # GRADIENT BANDIT\n",
    "        if self.gradient:\n",
    "            exp_est = np.exp(self.q_estimation) ## numerical preference exp^(H_t(a))\n",
    "            self.action_prob = exp_est / np.sum(exp_est) ## Probability of taking action a at time t; pi_t(a)\n",
    "            return np.random.choice(self.indices, p=self.action_prob) ## returns the action\n",
    "\n",
    "        q_best = np.max(self.q_estimation) ## choose the highest estimated Q value\n",
    "        return np.random.choice(np.where(self.q_estimation == q_best)[0]) # select the action with the corresponding estimated value\n",
    "\n",
    "    # take an action, update estimation for this action\n",
    "    def step(self, action):\n",
    "        # generate the reward under N(real reward, 1)\n",
    "        reward = np.random.randn() + self.q_true[action] ## R_t\n",
    "        self.time += 1 ## t\n",
    "        self.action_count[action] += 1 ## N_t(a) Number of times the action 'a' was selected.\n",
    "        self.average_reward += (reward - self.average_reward) / self.time ## R_t bar\n",
    "\n",
    "        if self.sample_averages:\n",
    "            # update estimation using sample averages\n",
    "            self.q_estimation[action] += (reward - self.q_estimation[action]) / self.action_count[action] ## Estimated value Q_t(a)\n",
    "        elif self.gradient:\n",
    "            one_hot = np.zeros(self.k)\n",
    "            one_hot[action] = 1\n",
    "            if self.gradient_baseline:\n",
    "                baseline = self.average_reward\n",
    "            else:\n",
    "                baseline = 0\n",
    "            self.q_estimation += self.step_size * (reward - baseline) * (one_hot - self.action_prob) # stochastic gradient ascent\n",
    "        else:\n",
    "            # update estimation with constant step size\n",
    "            self.q_estimation[action] += self.step_size * (reward - self.q_estimation[action]) # Tracking Nonstationary Problem\n",
    "        return reward\n",
    "\n",
    "## 10-armed bandit simulation\n",
    "def simulate(runs, time, bandits):\n",
    "    rewards = np.zeros((len(bandits), runs, time))\n",
    "    best_action_counts = np.zeros(rewards.shape)\n",
    "    for i, bandit in enumerate(bandits):\n",
    "        for r in trange(runs):\n",
    "            bandit.reset()\n",
    "            for t in range(time):\n",
    "                action = bandit.act()\n",
    "                reward = bandit.step(action)\n",
    "                rewards[i, r, t] = reward\n",
    "                if action == bandit.best_action:\n",
    "                    best_action_counts[i, r, t] = 1\n",
    "    mean_best_action_counts = best_action_counts.mean(axis=1)\n",
    "    mean_rewards = rewards.mean(axis=1)\n",
    "    return mean_best_action_counts, mean_rewards\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E-greedy plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def figure1(runs=2000, time=1000):\n",
    "    epsilons = [0, 0.1, 0.01] \n",
    "    bandits = [Bandit(epsilon=eps, sample_averages=True) for eps in epsilons]\n",
    "    best_action_counts, rewards = simulate(runs, time, bandits)\n",
    "\n",
    "    for eps, rewards in zip(epsilons, rewards):\n",
    "        plt.plot(rewards, label='epsilon = %.02f' % (eps))\n",
    "    plt.xlabel('steps')\n",
    "    plt.ylabel('average reward')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "#figure1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decaying e-greedy vs E-greedy plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def figure2(runs=2000, time=1000):\n",
    "    bandits = []\n",
    "    bandits.append(Bandit(decay2=True, sample_averages=True)) \n",
    "    bandits.append(Bandit(decay_eps=0, sample_averages=True,decay=True))\n",
    "    bandits.append(Bandit(decay1=True, sample_averages=True,total_time=time))\n",
    "    bandits.append(Bandit(epsilon=0.1, sample_averages=True))\n",
    "    best_action_counts,rewards = simulate(runs, time, bandits)\n",
    "\n",
    "    labels = [\"decay_eps=Function 1\",\"decay_eps=Function 2\",\"decay_eps=Function 3\",\"eps=0.1\"]\n",
    "    for i in range(len(bandits)):\n",
    "        plt.plot(rewards[i], label=labels[i])\n",
    "        \n",
    "    plt.xlabel('steps')\n",
    "    plt.ylabel('average reward')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "#figure2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimistic initial values vs E-greedy plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def figure3(runs=2000, time=1000):\n",
    "    bandits = []\n",
    "    bandits.append(Bandit(epsilon=0, initial=5, step_size=0.1))\n",
    "    bandits.append(Bandit(epsilon=0.1, initial=0, step_size=0.1))\n",
    "    best_action_counts, rewards = simulate(runs, time, bandits)\n",
    "    \n",
    "    plt.plot(rewards[0], label='epsilon = 0, q = 5')\n",
    "    plt.plot(rewards[1], label='epsilon = 0.1, q = 0')\n",
    "\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Average rewards')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "#figure3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UCB plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def figure4(runs=2000, time=1000):\n",
    "    bandits = []\n",
    "    bandits.append(Bandit(epsilon=0, UCB_param=0.5, sample_averages=True))\n",
    "    bandits.append(Bandit(epsilon=0, UCB_param=1.0, sample_averages=True))\n",
    "    bandits.append(Bandit(epsilon=0, UCB_param=2.0, sample_averages=True))\n",
    "\n",
    "    best_action_counts, rewards= simulate(runs, time, bandits)\n",
    "\n",
    "    plt.plot(rewards[0], label='UCB c = 0.5')\n",
    "    plt.plot(rewards[1], label='UCB c = 1.0')\n",
    "    plt.plot(rewards[2], label='UCB c = 2.0')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Average reward')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "#figure4()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UCB vs E-greedy plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def figure4_1(runs=2000, time=1000):\n",
    "    bandits = []\n",
    "    bandits.append(Bandit(epsilon=0, UCB_param=2, sample_averages=True))\n",
    "    bandits.append(Bandit(epsilon=0.1, sample_averages=True))\n",
    "    best_action_counts, rewards = simulate(runs, time, bandits)\n",
    "\n",
    "    plt.plot(rewards[0], label='UCB c = 2')\n",
    "    plt.plot(rewards[1], label='epsilon greedy epsilon = 0.1')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Average reward')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "#figure4_1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient bandit plot (for alpha = 0.1 and 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def figure5(runs=2000, time=1000):\n",
    "    bandits = []\n",
    "    bandits.append(Bandit(gradient=True, step_size=0.1, gradient_baseline=True, true_reward=4))\n",
    "    bandits.append(Bandit(gradient=True, step_size=0.1, gradient_baseline=False, true_reward=4))\n",
    "    bandits.append(Bandit(gradient=True, step_size=0.01, gradient_baseline=True, true_reward=4))\n",
    "    bandits.append(Bandit(gradient=True, step_size=0.01, gradient_baseline=False, true_reward=4))\n",
    "    best_action_counts, _ = simulate(runs, time, bandits)\n",
    "    labels = ['alpha = 0.1, with baseline',\n",
    "              'alpha = 0.1, without baseline',\n",
    "              'alpha = 0.01, with baseline',\n",
    "              'alpha = 0.01, without baseline']\n",
    "\n",
    "    for i in range(len(bandits)):\n",
    "        plt.plot(best_action_counts[i], label=labels[i])\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('% Optimal action')\n",
    "    plt.legend()\n",
    "    plt.show()    \n",
    "#figure5()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient bandit plot 2 (for alpha = 0.1 and 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def figure5_1(runs=2000, time=1000):\n",
    "    bandits = []\n",
    "    bandits.append(Bandit(gradient=True, step_size=0.1, gradient_baseline=True, true_reward=4))\n",
    "    bandits.append(Bandit(gradient=True, step_size=0.1, gradient_baseline=False, true_reward=4))\n",
    "    bandits.append(Bandit(gradient=True, step_size=0.4, gradient_baseline=True, true_reward=4))\n",
    "    bandits.append(Bandit(gradient=True, step_size=0.4, gradient_baseline=False, true_reward=4))\n",
    "    best_action_counts, _ = simulate(runs, time, bandits)\n",
    "    labels = ['alpha = 0.1, with baseline',\n",
    "              'alpha = 0.1, without baseline',\n",
    "              'alpha = 0.4, with baseline',\n",
    "              'alpha = 0.4, without baseline']\n",
    "\n",
    "    for i in range(len(bandits)):\n",
    "        plt.plot(best_action_counts[i], label=labels[i])\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('% Optimal action')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "#figure5_1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient bandit vs E-greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def figure5_2(runs=2000, time=1000):\n",
    "    bandits = []\n",
    "    bandits.append(Bandit(epsilon=0.1, initial=0, step_size=0.1))\n",
    "    bandits.append(Bandit(gradient=True, step_size=0.1, gradient_baseline=True, true_reward=4))\n",
    "    bandits.append(Bandit(gradient=True, step_size=0.1, gradient_baseline=False, true_reward=4))\n",
    "    bandits.append(Bandit(gradient=True, step_size=0.4, gradient_baseline=True, true_reward=4))\n",
    "    bandits.append(Bandit(gradient=True, step_size=0.4, gradient_baseline=False, true_reward=4))\n",
    "    best_action_counts, rewards = simulate(runs, time, bandits)\n",
    "    labels = [\"epsilon greedy epsilon = 0.1\",'alpha = 0.1, with baseline',\n",
    "              'alpha = 0.1, without baseline',\n",
    "              'alpha = 0.4, with baseline',\n",
    "              'alpha = 0.4, without baseline']\n",
    "\n",
    "    for i in range(len(bandits)):\n",
    "        plt.plot(best_action_counts[i], label=labels[i])\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('% Optimal action')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "#figure5_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
